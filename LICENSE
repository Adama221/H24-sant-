# Import des bibliothèques nécessaires
import tensorflow as tf
from transformers import GPT2Tokenizer, TFGPT2Model
from flask import Flask, request, jsonify

# Initialisation de l'application Flask
app = Flask(__name__)

# Chargement du modèle GPT-4
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")
model = TFGPT2Model.from_pretrained("gpt2-large")

# Données sur la santé mondiale
health_data = {
    "better_health": {
        "estimate": "1.26 billion",
        "range": "[0.96 - 1.49 billion]",
        "description": "Supplémentaires bénéficieront d'une meilleure santé et un meilleur bien-être d'ici 2023 par rapport à 2018."
    },
    "universal_coverage": {
        "estimate": "477 million",
        "range": "[433 - 523 million]",
        "description": "Supplémentaires bénéficieront de services de santé essentiels et ne connaîtront pas de difficultés financières d'ici 2023 par rapport à 2018."
    },
    "emergency_protection": {
        "estimate": "690 million",
        "range": "[591 - 784 million]",
        "description": "Supplémentaires seront protégées contre les urgences sanitaires d'ici 2023 par rapport à 2018."
    }
}

# Fonction pour générer une réponse à partir du modèle GPT-4
def generate_response(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors='tf')
    output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Route pour l'API de chatbot
@app.route("/chatbot", methods=["POST"])
def chatbot():
    data = request.json
    patient_question = data["question"]
    response = generate_response(patient_question)
    return jsonify({"response": response})

# Route pour récupérer les données sur la santé mondiale
@app.route("/health_data", methods=["GET"])
def get_health_data():
    return jsonify(health_data)

if __name__ == "__main__":
    app.run(debug=True)
